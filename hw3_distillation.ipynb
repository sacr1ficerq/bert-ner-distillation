{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6c9d7e-8f48-41e9-94ab-5c3b1b717694",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "## Домашнее задание 3: Уменьшение размеров модели\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — __10 баллов__.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "__Мягкий дедлайн 24.10.25 23:59__ \\\n",
    "__Жесткий дедлайн 26.10.25 23:59__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит научиться решать задачу Named Entity Recognition (NER) на самом популярном датасете – [CoNLL-2003](https://paperswithcode.com/dataset/conll-2003). В вашем распоряжении будет предобученный BERT, который вам необходимо уменьшить с минимальными потерями в качестве до размера 20М параметров. Для этого вы самостоятельно реализуете факторизацию эмбеддингов, дистилляцию, шеринг параметров и так далее.\n",
    "\n",
    "В этом задании вам придется проводить довольно много экспериментов, поэтому мы рекомендуем не писать весь код в тетрадке, а завести разные файлы для отдельных логических блоков и скомпоновать все в виде проекта. Это позволит вашему ноутбуку не разрастаться и сильно облегчит задачу и вам, и проверяющим. Так же постарайтесь логгировать все ваши эксперименты в wandb, чтобы ничего не потерялось.\n",
    "\n",
    "### Оценивание\n",
    "Оценка за это домашнее задание будет формироваться из оценки за __задания__ и за __отчет__, в котором от вас требуется написать о проделанной работе. За отчет можно получить до 2-х баллов, однако в случае отсутствия отчета баллы за соответствующие задания не будут ставиться. Задания делятся на две части: _номерные_ и _на выбор_. За _номерные_ можно получить в сумме 6 баллов, за задания _на выбор_ можно получить до 14. То есть за все дз можно получить 22 балла (но не радуйтесь рано, это не так просто). Все, что вы наберете свыше 10, будет считаться бонусами.\n",
    "\n",
    "\n",
    "### О датасете\n",
    "\n",
    "Named Entity Recognition – это задача классификации токенов по классам сущностей. В CoNLL-2003 для именования сущностей используется маркировка **BIO** (Beggining, Inside, Outside), в которой метки означают следующее:\n",
    "\n",
    "- *B-{метка}* – начало сущности *{метка}*\n",
    "- *I-{метка}* – продолжнение сущности *{метка}*\n",
    "- *O* – не сущность\n",
    "\n",
    "Существуют так же и другие способы маркировки, например, BILUO. Почитать о них можно [тут](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) и [тут](https://www.youtube.com/watch?v=dQw4w9WgXcQ).\n",
    "\n",
    "Всего в датасете есть 9 разных меток.\n",
    "- O – слову не соответствует ни одна сущность.\n",
    "- B-PER/I-PER – слово или набор слов соответстует определенному _человеку_.\n",
    "- B-ORG/I-ORG – слово или набор слов соответстует определенной _организации_.\n",
    "- B-LOC/I-LOC – слово или набор слов соответстует определенной _локации_.\n",
    "- B-MISC/I-MISC – слово или набор слов соответстует сущности, которая не относится ни к одной из предыдущих. Например, национальность, произведение искусства, мероприятие и т.д.\n",
    "\n",
    "Приступим!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe56a70-a72d-40a0-9ae3-395ec6460657",
   "metadata": {},
   "source": [
    "Начнем с загрузки и предобработки датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef87648-86ab-4f81-9db3-5cb7f54c575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset =  load_dataset(\"eriktks/conll2003\", revision=\"convert/parquet\")\n",
    "\n",
    "dataset = dataset.remove_columns([\"id\", \"pos_tags\", \"chunk_tags\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c1a5b0-6ca1-4159-9ce6-cff88aca6b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b0e34d-edca-40bc-83ac-cff0c2872f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958cdff9-6ea1-4f7f-808b-dbe5620c27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "words = dataset[\"train\"][0][\"tokens\"]\n",
    "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    print(f'{words[i]}\\t{label_names[labels[i]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2312e-1335-4afa-a6e6-3cdde8515fe5",
   "metadata": {},
   "source": [
    "### Предобработка\n",
    "\n",
    "На протяжении всего домашнего задания мы будем использовать _cased_ версию BERT, то есть токенизатор будет учитывать регистр слов. Для задачи NER регистр важен, так как имена и названия организаций или предметов искусства часто пишутся с большой буквы, и будет глупо прятать от модели такую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05edcd4e-5360-41a8-b403-a9084d6a3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f64076-829a-49f1-af58-6fe60c66f965",
   "metadata": {},
   "source": [
    "При токенизации слова могут разделиться на несколько токенов (как слово `Fischler` из примера ниже), из-за чего появится несоответствие между числом токенов и меток. Это несоответствие нам придется устранить вручную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebc8789-0bba-4c96-aa1a-84403c93260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова:  ['Only', 'France', 'and', 'Britain', 'backed', 'Fischler', \"'s\", 'proposal', '.']\n",
      "Токены: ['[CLS]', 'Only', 'France', 'and', 'Britain', 'backed', 'Fi', '##sch', '##ler', \"'\", 's', 'proposal', '.', '[SEP]']\n",
      "Метки: ['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][12]\n",
    "words = example[\"tokens\"]\n",
    "tags = [label_names[t] for t in example[\"ner_tags\"]]\n",
    "tokenized_text = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "\n",
    "print('Слова: ', words)\n",
    "print('Токены:', tokenized_text.tokens())\n",
    "print('Метки:', tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34949bff-d7e9-47b3-aea7-82f1332a899c",
   "metadata": {},
   "source": [
    "__Задание 1 (1 балл).__ Токенизируйте весь датасет и для каждого текста выравните токены с метками так, чтобы каждому токену соответствовала одна метка. При этом важно сохранить нотацию BIO. И не забудьте про специальные токены! Должно получиться что-то такое:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e77b80-7e65-41a4-8986-aaac792385df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2809, 1699, 1105, 2855, 5534, 17355, 9022, 2879, 112, 188, 5835, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 5, 0, 5, 0, 1, 2, 2, 0, 0, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(example, tokenizer):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    original_labels = example[\"ner_tags\"]\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    b_to_i_map = {}\n",
    "    for i, label in enumerate(label_names):\n",
    "        if label.startswith(\"B-\"):\n",
    "            i_label = \"I-\" + label[2:]\n",
    "            if i_label in label_names:\n",
    "                b_to_i_map[i] = label_names.index(i_label)\n",
    "\n",
    "    new_labels = []\n",
    "    previous_word_id = None\n",
    "    for word_id in word_ids:\n",
    "        # Для специальных токенов ([CLS], [SEP]) устанавливаем -100\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        # Для первого токена нового слова используем исходную метку\n",
    "        elif word_id != previous_word_id:\n",
    "            new_labels.append(original_labels[word_id])\n",
    "        # Для последующих токенов того же слова\n",
    "        else:\n",
    "            original_label_id = original_labels[word_id]\n",
    "            # Если исходная метка была B-, заменяем её на I-\n",
    "            if original_label_id in b_to_i_map:\n",
    "                new_labels.append(b_to_i_map[original_label_id])\n",
    "            else:\n",
    "                # Если метка была 'O' или уже 'I-', оставляем как есть\n",
    "                new_labels.append(original_label_id)\n",
    "        \n",
    "        previous_word_id = word_id\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "a = tokenize_and_align_labels(example, tokenizer)\n",
    "print(a)\n",
    "# print(*zip(, tokenized_text.tokens()), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8352b8f-f60a-4844-b428-9e866678dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выровненные метки: [-100, 0, 5, 0, 5, 0, 1, 2, 2, 0, 0, 0, 0, -100]\n",
      "Выровненные названия меток: [-100, 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', -100]\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = tokenize_and_align_labels(example, tokenizer)['labels']\n",
    "tags = [label_names[t] if t > -1 else t for t in aligned_labels]\n",
    "print(\"Выровненные метки:\", aligned_labels)\n",
    "print(\"Выровненные названия меток:\", tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6b38fe-ea55-4526-8aef-9c1a76442fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93809cc7-4cde-4764-ab2d-6d9a76f879e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    # remove_columns=dataset.column_names['train'],\n",
    "    num_proc=6\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60048484-5849-4342-ab0c-898e68da4474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e9399-8393-4312-88aa-53b727d9df7d",
   "metadata": {},
   "source": [
    "### Метрика\n",
    "\n",
    "Для оценки качества NER обычно используют F1 меру с микро-усреднением. Мы загрузим ее из библиотеки `seqeval`. Функция `f1_score` принимает два 2d списка с правильными и предсказанными метками, записаными текстом, и возвращает для них значение F1. Вы можете использовать ее с параметрами по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3df54ab-c65b-40e0-b479-25d6f29e5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380833ce-1b8e-4b00-90ee-9126df16c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc901ecf-2de9-4c3f-862c-cf78871d8d9f",
   "metadata": {},
   "source": [
    "Особенность подсчета F1 для NER заключается в том, что в некоторых ситуациях неправильные ответы могут засчитываться как правильные. Например, если модель предсказала `['I-PER', 'I-PER']`, то мы можем догадаться, что на самом деле должно быть `['B-PER', 'I-PER']`, так как сущность не может начинаться с `I-`. Функция `f1_score` учитывает это и поэтому работает только с текстовыми представлениями меток."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61400bf-712a-4dfb-a08f-326c5db10eb2",
   "metadata": {},
   "source": [
    "### Модель\n",
    "\n",
    "В качестве базовой модели мы возьмем `bert-base-cased`. Как вы понимаете, он не обучался на задачу NER. Поэтому прежде чем приступать к уменьшению размера BERT, его необходимо дообучить.\n",
    "\n",
    "__Задание 2 (1 балл)__ Дообучите `bert-base-cased` на нашем датасете с помощью обычного fine-tuning. У вас должно получиться хотя бы 0.9 F1 на тестовой выборке. Заметьте, что чем выше качество большой модели, тем лучше будет работать дистиллированный ученик. Для обучения можно использовать `Trainer` из Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14235f7c-d7a3-4407-98fe-be35bec84008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число параметров: 107726601\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_names))\n",
    "\n",
    "print('Число параметров:', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34dfac9f-2d47-489d-acdb-54ec64da7145",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [label_names[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_names[p] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01609e0d-3d2c-426c-bc20-61c0dd98cc1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ae197eb-041a-4cdb-864d-69b9cf099be9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='814' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [407/407 02:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.028104066848755,\n",
       " 'eval_model_preparation_time': 0.0041,\n",
       " 'eval_precision': 0.007460520204366001,\n",
       " 'eval_recall': 0.0432514304947829,\n",
       " 'eval_f1': 0.01272592225798465,\n",
       " 'eval_runtime': 10.3656,\n",
       " 'eval_samples_per_second': 313.538,\n",
       " 'eval_steps_per_second': 39.265}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "650f0726-1f68-4fb8-a5d2-db1cf586f700",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 07:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.110300</td>\n",
       "      <td>0.062381</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.911562</td>\n",
       "      <td>0.936722</td>\n",
       "      <td>0.923971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.064062</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.930298</td>\n",
       "      <td>0.945641</td>\n",
       "      <td>0.937907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.060913</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.931615</td>\n",
       "      <td>0.949175</td>\n",
       "      <td>0.940313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.06636513072752302, metrics={'train_runtime': 426.6264, 'train_samples_per_second': 98.735, 'train_steps_per_second': 12.348, 'total_flos': 920771584279074.0, 'train_loss': 0.06636513072752302, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cda5a7-7fb9-43bc-84e3-e66ec6a48d91",
   "metadata": {},
   "source": [
    "### Факторизация матрицы эмбеддингов\n",
    "\n",
    "Можно заметить, что на данный момент матрица эмбеддингов занимает $V \\cdot H = 28996 \\cdot 768 = 22.268.928$ параметров. Это aж пятая часть от всей модели! Давайте попробуем что-то с этим сделать. В модели [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) предлагается факторизовать матрицу эмбеддингов в произведение двух небольших матриц. Таким образом, параметры эмбеддингов будут содержать $V \\cdot E + E \\cdot H$ элементов, что гораздо меньше $V \\cdot H$, если $H \\gg E$. Авторы выбирают $E = 128$, однако ничего не мешает нам взять любое другое значение. Например, выбрав $H = 64$, мы уменьшим число параметров примерно на 20М.\n",
    "\n",
    "__Задание 3 (1 балл).__ Напишите класс-обертку над слоем эмбеддингов, который реализует факторизацию на две матрицы, и дообучите факторизованную модель. Заметьте, обе матрицы можно инициализировать с помощью SVD разложения, чтобы начальное приближение было хорошим. Это сэкономит очень много времени на дообучении. С рангом разложения $H = 64$ у вас должно получиться F1 больше 0.87."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1639dac-cf3c-4312-8330-d4f357f38c0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "472d9e33-4dee-4e9a-baff-44682f0655cb",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c4f3bfd-a3a9-4468-b3df-eaa28a49ec96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "W = model.bert.embeddings.word_embeddings.weight\n",
    "u, s, v = torch.linalg.svd(W, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75e0099b-4eff-4eb9-9fb3-a857505aee8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(s[:].detach().cpu().numpy(), bins=80)\n",
    "plt.axvline(s[64].detach().cpu().numpy(), color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0b3b371-9e56-4fa4-95db-4ee8b92f5f17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4977, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bcbc145-5925-45b1-98a0-f51958b91ba1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8298, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[64:].sum() / s.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c1e8bd-9fe0-4955-a710-bab67fd5dc8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f01feef9-2162-4bcd-bb9b-59328d23981a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class EmbeddingWrapper(nn.Module):\n",
    "    def __init__(self, big_embeddings: nn.Embedding, rank: int):\n",
    "        super().__init__()\n",
    "        vocab_size = big_embeddings.num_embeddings\n",
    "        output_size = big_embeddings.embedding_dim\n",
    "\n",
    "        # init using SVD\n",
    "        UT, S, V = torch.linalg.svd(big_embeddings.weight, full_matrices=False)\n",
    "        s = torch.sqrt(S[:rank])\n",
    "        A = UT[:, :rank] * s\n",
    "        B = torch.diag(s) @ V[:rank, :]\n",
    "\n",
    "        # print(A.shape)\n",
    "        # print(B.shape)\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=rank, padding_idx=big_embeddings.padding_idx)\n",
    "        self.embedding.weight.data = A\n",
    "\n",
    "        self.projection = nn.Linear(in_features=rank, out_features=output_size, bias=False)\n",
    "        self.projection.weight.data = B.T\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98fb8932-ed58-4fd9-bf41-4ed3eec0976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.bert.embeddings.word_embeddings = original_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c84ad3f-da0d-4a90-8a06-6ec78a32ece8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank = 64\n",
    "original_embeddings = model.bert.embeddings.word_embeddings\n",
    "factorized_embeddings = EmbeddingWrapper(original_embeddings, rank=rank)\n",
    "model.bert.embeddings.word_embeddings = factorized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf1ee1-0ffb-4505-a819-aeb7548b23b7",
   "metadata": {},
   "source": [
    "##### print(f'Число параметров: {sum(p.numel() for p in model.parameters()) / 1e6:0.2f}M', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29364db4-0370-49cb-84d3-8ea3446352c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dbac349-8d5a-4764-9653-be37bf2b7c21",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='814' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [407/407 02:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1365939378738403,\n",
       " 'eval_model_preparation_time': 0.0078,\n",
       " 'eval_precision': 0.4466243508366994,\n",
       " 'eval_recall': 0.13025917199596096,\n",
       " 'eval_f1': 0.20169381107491857,\n",
       " 'eval_runtime': 7.0967,\n",
       " 'eval_samples_per_second': 457.957,\n",
       " 'eval_steps_per_second': 57.35}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adb82e08-07c3-410b-accd-032109a1acdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 06:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.097016</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.888616</td>\n",
       "      <td>0.912992</td>\n",
       "      <td>0.900639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.104698</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.908805</td>\n",
       "      <td>0.924100</td>\n",
       "      <td>0.916389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.105277</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.916846</td>\n",
       "      <td>0.931505</td>\n",
       "      <td>0.924117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.062418800225780516, metrics={'train_runtime': 385.922, 'train_samples_per_second': 109.149, 'train_steps_per_second': 13.65, 'total_flos': 921303634723362.0, 'train_loss': 0.062418800225780516, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220bd15-3681-4006-b7e0-44838b3500ad",
   "metadata": {},
   "source": [
    "### Дистилляция знаний\n",
    "\n",
    "Дистилляция знаний – это парадигма обучения, в которой знания модели-учителя дистиллируются в модель-ученика. Учеником может быть произвольная модель меньшего размера, решающая ту же задачу, однако обычно ученик имеет ту же архитектуру, что и учитель. При дистилляции используются два функционала ошибки:\n",
    "\n",
    "1. Стандартная кросс-энтропия.\n",
    "1. Функция, задающая расстояние между распределениями предсказаний учителя и ученика. Чаще всего используют KL-дивергенцию.\n",
    "\n",
    "Для того, чтобы распределение предсказаний учителя не было вырожденным, к softmax добавляют температуру больше 1, например, 2 или 5.   \n",
    "__Важно:__ при делении логитов на температуру значения градиентов уменьшаются в $\\tau^2$ раз (проверьте это!). Поэтому для возвращения их в изначальный масштаб ошибку надо домножить на $\\tau^2$. Подробнее об этом можно почитать в разделе 2.1 [оригинальной статьи](https://arxiv.org/pdf/1503.02531).\n",
    "\n",
    "<img src=\"https://intellabs.github.io/distiller/imgs/knowledge_distillation.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e921f-279e-46ae-8c70-5d715b91106e",
   "metadata": {},
   "source": [
    "__Задание 4 (3 балла).__ Реализуйте метод дистилляции знаний, изображенный на картинке. Для подсчета ошибки между предсказаниями ученика и учителя используйте KL-дивергенцию [`nn.KLDivLoss(reduction=\"batchmean\")`](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) (обратите внимание на вормат ее входов). Для получения итоговой ошибки суммируйте мягкую ошибку с жесткой.   \n",
    "В качестве учителя используйте дообученный BERT из задания 2. В качестве ученика возьмите необученную модель с размером __не больше 20M__ параметров. Вы можете использовать факторизацию матрицы эмбеддингов для уменьшения числа параметров. Если вы все сделали правильно, то на тестовой выборке вы должны получить значение F1 не меньше 0.7. Вам должно хватить примерно 20к итераций обучения для этого. Если у вас что-то не получается, то можно ориентироваться на статью про [DistilBERT](https://arxiv.org/abs/1910.01108) и на [эту статью](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT).\n",
    "\n",
    "__Важно:__\n",
    "* Не забывайте добавлять _warmup_ при обучении ученика.\n",
    "* Не забывайте переводить учителя в режим _eval_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d092614-86b8-41c4-ad77-6c6df9edc822",
   "metadata": {},
   "source": [
    "#### Prepare teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42d09853-5c30-4d3b-ab4e-515311aef21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.embeddings.word_embeddings = original_embeddings\n",
    "teacher = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "517ce98c-7dfe-4c73-8d26-ba10df497029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3dd3475-89c7-4491-9716-7758815db4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in teacher.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b503378f-3fda-4bf8-9405-0a7a1f72fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in teacher.parameters():\n",
    "    assert param.requires_grad == False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aec466-249c-4ceb-833e-8dae5ae3fa67",
   "metadata": {},
   "source": [
    "#### Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "144c672c-cacb-4a83-a433-f543051b13be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea87f684-a96f-4c62-941f-f0caa65b7eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3072 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2ece1172-0a98-4f17-8ee8-7edc17cc3d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student model created with 16,277,641 parameters.\n",
      "Student model configuration: BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForTokenClassification\n",
    "\n",
    "student_config = BertConfig(\n",
    "    vocab_size=28996,\n",
    "    hidden_size=768//2,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=3072//2,\n",
    "    num_labels=9,\n",
    ")\n",
    "\n",
    "student = BertForTokenClassification(student_config)\n",
    "\n",
    "big_embeddings = student.bert.embeddings.word_embeddings\n",
    "smol_embeddings = EmbeddingWrapper(big_embeddings, rank=64)\n",
    "student.bert.embeddings.word_embeddings = smol_embeddings\n",
    "\n",
    "total_params = sum(p.numel() for p in student.parameters())\n",
    "print(f\"Student model created with {total_params:,} parameters.\")\n",
    "print(f\"Student model configuration: {student_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ed65e5a3-e7eb-4d43-8eef-6b1a84761ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): EmbeddingWrapper(\n",
       "        (embedding): Embedding(28996, 64, padding_idx=0)\n",
       "        (projection): Linear(in_features=64, out_features=384, bias=False)\n",
       "      )\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-7): 8 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=384, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "da9d0f9d-c328-4d64-a131-cc2ac49c0d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ac22f-2fe5-49a2-9ede-7a2d6b380e4f",
   "metadata": {},
   "source": [
    "#### DistillationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "346cdf5f-a253-4884-bbbb-246409d8891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.teacher_model.eval()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # print(inputs.keys())\n",
    "        # print(*map(lambda x: x.shape, inputs.values()))\n",
    "        # print(student_logits.shape, teacher_logits.shape)\n",
    "        \n",
    "        # Reshape for cross_entropy\n",
    "        num_classes = student_logits.shape[-1]\n",
    "        logits_for_loss = student_logits.view(-1, num_classes)\n",
    "        labels_for_loss = inputs['labels'].view(-1)\n",
    "\n",
    "        # hard loss\n",
    "        hard_loss = F.cross_entropy(logits_for_loss, labels_for_loss)\n",
    "\n",
    "        # soft loss\n",
    "        soft_labels = F.softmax(teacher_logits / self.temperature, dim=-1)\n",
    "        # soft predictions (log-probs for KL loss)\n",
    "        soft_predictions = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
    "\n",
    "        distillation_loss = F.kl_div(soft_predictions, soft_labels, reduction='batchmean') * (self.temperature ** 2)\n",
    "        total_loss = self.alpha * distillation_loss + (1 - self.alpha) * hard_loss\n",
    "\n",
    "        return (total_loss, student_outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "da82dddb-0f98-4ddd-a796-2fb760f1a532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101,\n",
      "               7270,\n",
      "               22961,\n",
      "               1528,\n",
      "               1840,\n",
      "               1106,\n",
      "               21423,\n",
      "               1418,\n",
      "               2495,\n",
      "               12913,\n",
      "               119,\n",
      "               102],\n",
      " 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100],\n",
      " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      " 'tokens': ['EU',\n",
      "            'rejects',\n",
      "            'German',\n",
      "            'call',\n",
      "            'to',\n",
      "            'boycott',\n",
      "            'British',\n",
      "            'lamb',\n",
      "            '.']}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for x in tokenized_dataset['train']:\n",
    "    pprint(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756e812-232c-4b6f-85f0-ffb039bc07e2",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4726c68-2371-4efe-859b-412c19d83a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7ac81-fdeb-4a32-b427-e278cb288d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18419599-aae9-48d3-b6a8-21452bf27733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8f571ae8-27d8-4bf8-9e5b-fc5114ec2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    warmup_steps=1000,\n",
    "    dataloader_num_workers=6,\n",
    "    run_name=\"Distillation\",\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy='epoch',\n",
    "    eval_on_start=True,\n",
    "    save_strategy='epoch',\n",
    "    # eval_steps=5_000,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    teacher_model=teacher,\n",
    "    temperature=2.0,\n",
    "    alpha=0.5,\n",
    "    model=student,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# add warmup stage to student\n",
    "# put teacher in eval and train two model together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b13a35d0-f84f-433b-82cd-0c21fadbbb47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21072' max='21072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21072/21072 14:10, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>163.293198</td>\n",
       "      <td>0.010612</td>\n",
       "      <td>0.095254</td>\n",
       "      <td>0.019096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23.381000</td>\n",
       "      <td>20.625509</td>\n",
       "      <td>0.386037</td>\n",
       "      <td>0.488556</td>\n",
       "      <td>0.431288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.490600</td>\n",
       "      <td>15.528271</td>\n",
       "      <td>0.561052</td>\n",
       "      <td>0.635645</td>\n",
       "      <td>0.596023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.676500</td>\n",
       "      <td>13.166449</td>\n",
       "      <td>0.638567</td>\n",
       "      <td>0.704981</td>\n",
       "      <td>0.670133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.344300</td>\n",
       "      <td>11.820036</td>\n",
       "      <td>0.652734</td>\n",
       "      <td>0.733255</td>\n",
       "      <td>0.690655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.968400</td>\n",
       "      <td>11.365520</td>\n",
       "      <td>0.694972</td>\n",
       "      <td>0.763043</td>\n",
       "      <td>0.727419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.119100</td>\n",
       "      <td>11.017979</td>\n",
       "      <td>0.711228</td>\n",
       "      <td>0.771794</td>\n",
       "      <td>0.740274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.545800</td>\n",
       "      <td>11.024203</td>\n",
       "      <td>0.726699</td>\n",
       "      <td>0.775496</td>\n",
       "      <td>0.750305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.037700</td>\n",
       "      <td>11.046603</td>\n",
       "      <td>0.718279</td>\n",
       "      <td>0.778357</td>\n",
       "      <td>0.747113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.505200</td>\n",
       "      <td>11.161644</td>\n",
       "      <td>0.720254</td>\n",
       "      <td>0.783406</td>\n",
       "      <td>0.750504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.537800</td>\n",
       "      <td>10.746635</td>\n",
       "      <td>0.734945</td>\n",
       "      <td>0.782565</td>\n",
       "      <td>0.758008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.221000</td>\n",
       "      <td>10.600514</td>\n",
       "      <td>0.736207</td>\n",
       "      <td>0.790475</td>\n",
       "      <td>0.762376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.447800</td>\n",
       "      <td>10.525409</td>\n",
       "      <td>0.741701</td>\n",
       "      <td>0.789633</td>\n",
       "      <td>0.764917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21072, training_loss=10.122965477005314, metrics={'train_runtime': 860.392, 'train_samples_per_second': 195.832, 'train_steps_per_second': 24.491, 'total_flos': 616680694393434.0, 'train_loss': 10.122965477005314, 'epoch': 12.0})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159870d-4dc7-4e60-89b2-bf99c48b8ee9",
   "metadata": {},
   "source": [
    "# Задания на выбор\n",
    "\n",
    "Как вы понимаете, есть еще довольно много разных способов уменьшить обученную модель. В этой секции вам предлагается реализовать разные техники на выбор. За каждую из них можно получить разное количество балов в зависимости от сложности. Успешность реализации будет оцениваться как по коду, так и по качеству на тестовой выборке. Все баллы за это дз, которые вы наберете сверх 10, будут считаться бонусными.   \n",
    "В задании 4 вы обучали модель с ограничением числа параметров в 20М. При реализации техник из этой секции придерживайтесь такого же ограничения. Это позволит честно сравнивать методы между собой и делать правильные выводы. Напишите в отчете обо всем, что вы попробовали.\n",
    "\n",
    "* __Шеринг весов (1 балл).__ В модификации BERT [ALBERT](https://arxiv.org/pdf/1909.11942.pdf) помимо факторизации эмбеддингов предлагается шерить веса между слоями. То есть разные слои используют одни и те же веса. Такая техника эвивалентна применению одного и того же слоя несколько раз. Она позволяет в несколько раз уменьшить число параметров и не сильно потерять в качестве.\n",
    "* __Факторизация промежуточных слоев (1 балл).__ Если можно факторизовать матрицу эмбеддингов, то и все остальное тоже можно. Для факторизации слоев существует много разных подходов и выбрать какой-то один сложно. Вы можете вдохновляться [этим списком](https://lechnowak.com/posts/neural-network-low-rank-factorization-techniques/), найти в интернете что-то другое или придумать метод самостоятельно. В любом случае в отчете обоснуйте, почему вы решили сделать так как сделали.\n",
    "* __Приближение промежуточных слоев (2 балла).__ Мы обсуждали, что помимо приближения выходов модели ученика к выходам модели учителя, можно приближать выходы промежуточных слоев. В [этой работе](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT) подробно написано, как это можно сделать.\n",
    "* __Прунинг (4 балла).__ В методе [SparseGPT](https://arxiv.org/abs/2301.00774) предлагается подход, удаляющий веса модели один раз после обучения. При этом оказывается возможным удалить до половины всех весов без потери в качестве. Математика, стоящаяя за техникой, довольно сложная, однако общий подход простой – будем удалять веса в каждом слое по отдельности, при удалении части весов слоя, остальные веса будут перенастраиваться так, чтобы общий выход слоя не изменился.\n",
    "* __Удаление голов (6 баллов).__ В данный момент мы используем все головы внимания, но ряд исследований показывает, что большинство из них можно выбросить без потери качества. В этой [статье](https://arxiv.org/pdf/1905.09418.pdf) предлагается подход, который добавляет гейты к механизму внимания, которые регулируют, какие головы участвуют в слое, а какие – нет. В процессе обучения гейты настраиваются так, чтобы большинство голов не использовалась. В конце обучения неиспользуемые головы можно удалить. За это задание дается много баллов, потому что в методе довольно сложная математика и подход плохо заводится. Если вы решитесь потратить на него свои силы, то в случае неудачи мы дадим промежуточные баллы, опираясь на отчет.   \n",
    "__Совет:__ во время обучения внимательно следите за поведением гейтов. Если вы все сделали правильно, то они должны зануляться. Однако зануляются они не всегда сразу, им надо дать время и обучать модель подольше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48496261-199c-4009-8315-96960c984a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) add weight factorization( check for something more sophisticated then sumple wrappers)\n",
    "# 2) add attention loss and adaptive soft/hard loss parameter\n",
    "# 3) weight sharing? read ALBERT and see whats up with that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88aedb9-3375-47be-a702-9f23a8824830",
   "metadata": {},
   "source": [
    "### Weight sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "39e62413-912e-4cac-b242-e7466880677b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e54d8-207e-426f-92f1-a754bd9d3b67",
   "metadata": {},
   "source": [
    "Я хочу попробовать арзитектуру предложенную в ALBERT полностью, поэтому обучу модель с факторизацией эмбедингов отдельно, чтобы появилась точка отсчета. Потом добавлю шеринг параметров attention, что привело к лучшим результатам в статье, потом еще больше уменьшу модель через шеринг параметров FFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7bba723a-7277-462a-9c8d-b510704336b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число параметров: 107,726,601\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_names))\n",
    "print(f'Число параметров: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fe116c9e-721e-4250-a4ce-c8dc2f1acadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число параметров: 89,267,465\n"
     ]
    }
   ],
   "source": [
    "rank = 128\n",
    "original_embeddings = model.bert.embeddings.word_embeddings\n",
    "factorized_embeddings = EmbeddingWrapper(original_embeddings, rank=rank)\n",
    "model.bert.embeddings.word_embeddings = factorized_embeddings\n",
    "print(f'Число параметров: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "74892610-e068-4631-8180-87acf9729c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.1,\n",
    "    bf16=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    eval_on_start=True,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ae523c23-b1d3-4934-b673-38c66ede1dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 07:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.323120</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>0.058061</td>\n",
       "      <td>0.018683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.090316</td>\n",
       "      <td>0.878164</td>\n",
       "      <td>0.910973</td>\n",
       "      <td>0.894267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.089452</td>\n",
       "      <td>0.913470</td>\n",
       "      <td>0.925614</td>\n",
       "      <td>0.919502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.087978</td>\n",
       "      <td>0.916061</td>\n",
       "      <td>0.933019</td>\n",
       "      <td>0.924462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.09656790837620073, metrics={'train_runtime': 487.0179, 'train_samples_per_second': 86.492, 'train_steps_per_second': 10.817, 'total_flos': 921835685167650.0, 'train_loss': 0.09656790837620073, 'epoch': 3.0})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94876f-8bcc-4c3c-b549-e3428b05a1ba",
   "metadata": {},
   "source": [
    "#### Attention sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "314b3d19-e166-4c47-bcc4-0cab8ebbf056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число параметров base: 107,726,601\n",
      "Число параметров embedding factorized: 89,267,465\n",
      "Число параметров shared attention: 63,264,521\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_names))\n",
    "print(f'Число параметров base: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "rank = 128\n",
    "original_embeddings = model.bert.embeddings.word_embeddings\n",
    "factorized_embeddings = EmbeddingWrapper(original_embeddings, rank=rank)\n",
    "model.bert.embeddings.word_embeddings = factorized_embeddings\n",
    "print(f'Число параметров embedding factorized: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "shared_attention = model.bert.encoder.layer[0].attention\n",
    "\n",
    "for i in range(1, len(model.bert.encoder.layer)):\n",
    "    model.bert.encoder.layer[i].attention = shared_attention\n",
    "\n",
    "print(f'Число параметров shared attention: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "be2bba43-7e5a-4b30-8585-9424dddaf3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=1000,\n",
    "    dataloader_num_workers=10,\n",
    "    weight_decay=0.1,\n",
    "    bf16=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    eval_on_start=True,\n",
    "    report_to='tensorboard',\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f284dbc7-8216-4960-80aa-f8819a961598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 06:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.161927</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.039212</td>\n",
       "      <td>0.006307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.204727</td>\n",
       "      <td>0.683708</td>\n",
       "      <td>0.771962</td>\n",
       "      <td>0.725160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>0.115697</td>\n",
       "      <td>0.820622</td>\n",
       "      <td>0.874621</td>\n",
       "      <td>0.846762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.116228</td>\n",
       "      <td>0.852483</td>\n",
       "      <td>0.892797</td>\n",
       "      <td>0.872174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.22283507705062922, metrics={'train_runtime': 423.0602, 'train_samples_per_second': 99.567, 'train_steps_per_second': 12.452, 'total_flos': 640364373562914.0, 'train_loss': 0.22283507705062922, 'epoch': 3.0})"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64585cab-4d0d-4714-b070-91e6e922cd56",
   "metadata": {},
   "source": [
    "#### FFN & attention weights sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "70e331a3-d691-4b71-bff5-39037a3d05b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEncoder(\n",
       "  (layer): ModuleList(\n",
       "    (0-11): 12 x BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSdpaSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (intermediate_act_fn): GELUActivation()\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f3ab4ffd-9aad-4efe-b8a4-0675b5b33b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число параметров base: 107,726,601\n",
      "Число параметров embedding factorized: 89,267,465\n",
      "Число параметров shared everything: 11,300,873\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(label_names))\n",
    "print(f'Число параметров base: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "rank = 128\n",
    "original_embeddings = model.bert.embeddings.word_embeddings\n",
    "factorized_embeddings = EmbeddingWrapper(original_embeddings, rank=rank)\n",
    "model.bert.embeddings.word_embeddings = factorized_embeddings\n",
    "print(f'Число параметров embedding factorized: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "for i in range(1, len(model.bert.encoder.layer)):\n",
    "    model.bert.encoder.layer[i].attention = model.bert.encoder.layer[0].attention\n",
    "    model.bert.encoder.layer[i].intermediate = model.bert.encoder.layer[0].intermediate\n",
    "    model.bert.encoder.layer[i].output = model.bert.encoder.layer[0].output\n",
    "\n",
    "print(f'Число параметров shared everything: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0eab4249-d0b2-4bf4-a540-fc456f170fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 05:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.156683</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.198619</td>\n",
       "      <td>0.681296</td>\n",
       "      <td>0.768092</td>\n",
       "      <td>0.722095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.138094</td>\n",
       "      <td>0.798362</td>\n",
       "      <td>0.852911</td>\n",
       "      <td>0.824736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.128125</td>\n",
       "      <td>0.824920</td>\n",
       "      <td>0.869068</td>\n",
       "      <td>0.846419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.22478571551411372, metrics={'train_runtime': 333.8479, 'train_samples_per_second': 126.174, 'train_steps_per_second': 15.78, 'total_flos': 77878981204002.0, 'train_loss': 0.22478571551411372, 'epoch': 3.0})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=1000,\n",
    "    dataloader_num_workers=10,\n",
    "    weight_decay=0.1,\n",
    "    bf16=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    eval_on_start=True,\n",
    "    report_to='tensorboard',\n",
    "    save_safetensors=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70df66-b580-435b-95f0-6a84d51cd886",
   "metadata": {},
   "source": [
    "Получается, что оставив меньше 1/10 модели, можно все равно иметь не худший перформанс."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebc35b-c8eb-4825-9d0a-0a572f3986f8",
   "metadata": {},
   "source": [
    "### Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c9aa4-fd92-4080-b0c4-1486ed70866e",
   "metadata": {},
   "source": [
    "В этом ноутбуке все эксперименты были поставленны наугад и с правильными гиперараметрами методы показали бы другие результаты. Однако, я не думаю, что по моей работе в любом случае можно было бы судить про методы, я бы лучше доверял ALBERT, где показывается, что результаты почти не меняются от шеринга весов attention.\n",
    "\n",
    "Спасибо за эту HW! Было очень интересно порезать разными способами BERT. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-HW",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
