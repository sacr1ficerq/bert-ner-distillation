{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a00e95-98eb-48e8-bc39-06d985d707b4",
   "metadata": {},
   "source": [
    "# HW3 report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894a638-1507-47ae-bee6-c00e49ef5e26",
   "metadata": {},
   "source": [
    "## Embedding factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1bb3d58-3354-4829-8653-f8c021e60c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Dataset ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Model and Applying Factorization ---\n",
      "Original parameter count: 107,726,601\n",
      "cpu\n",
      "Factorized parameter count (rank=64): 87,362,569\n",
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8780' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8780/8780 04:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.117440</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>0.037698</td>\n",
       "      <td>0.010368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.149100</td>\n",
       "      <td>0.110965</td>\n",
       "      <td>0.847373</td>\n",
       "      <td>0.882026</td>\n",
       "      <td>0.864352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.130066</td>\n",
       "      <td>0.903204</td>\n",
       "      <td>0.906092</td>\n",
       "      <td>0.904646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.101623</td>\n",
       "      <td>0.903364</td>\n",
       "      <td>0.921912</td>\n",
       "      <td>0.912544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.099494</td>\n",
       "      <td>0.914262</td>\n",
       "      <td>0.927802</td>\n",
       "      <td>0.920982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.116123</td>\n",
       "      <td>0.912512</td>\n",
       "      <td>0.930326</td>\n",
       "      <td>0.921333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1026 23:30:53.994000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/6] _maybe_guard_rel() was called on non-relation expression Eq(s33, s92) | Eq(s92, 1)\n",
      "W1026 23:30:53.996000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/6] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "W1026 23:31:26.761000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/7] _maybe_guard_rel() was called on non-relation expression Eq(s33, s92) | Eq(s92, 1)\n",
      "W1026 23:31:26.763000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/7] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "W1026 23:32:31.039000 92806 torch/_dynamo/convert_frame.py:1358] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1026 23:32:31.039000 92806 torch/_dynamo/convert_frame.py:1358] [0/8]    function: 'forward' (/home/user/Desktop/venv/lib/python3.10/site-packages/accelerate/utils/operations.py:818)\n",
      "W1026 23:32:31.039000 92806 torch/_dynamo/convert_frame.py:1358] [0/8]    last reason: 0/6: 2 <= kwargs['input_ids'].size()[0]  # if token_type_ids is None:  # transformers/models/bert/modeling_bert.py:1070 in forward (user code shown is first use of this value--the guard itself is not due user code but due to 0/1 specialization in the framework; to avoid specialization try torch._dynamo.mark_unbacked(tensor, dim))\n",
      "W1026 23:32:31.039000 92806 torch/_dynamo/convert_frame.py:1358] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1026 23:32:31.039000 92806 torch/_dynamo/convert_frame.py:1358] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Finished ---\n",
      "\n",
      "========================================\n",
      "           EXPERIMENT RESULTS\n",
      "========================================\n",
      "\n",
      "--- Final Evaluation on Test Set ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on the unseen test set:\n",
      "  - Test loss: 0.2710\n",
      "  - Test precision: 0.8546\n",
      "  - Test recall: 0.8886\n",
      "  - Test f1: 0.8713\n",
      "  - Test runtime: 3.9018\n",
      "  - Test samples_per_second: 884.9700\n",
      "  - Test steps_per_second: 110.7170\n"
     ]
    }
   ],
   "source": [
    "import embedding_factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc59d8-2bc2-469e-ba36-1e310c74fc01",
   "metadata": {},
   "source": [
    "## Parameter sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02c47a-abd3-47db-9c31-5e8e1add38f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attention sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e95b1c3-19a8-4514-9dcf-9a909d8b9953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Dataset ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Model and Applying Factorization ---\n",
      "Original parameter count: 107,726,601\n",
      "Factorized parameter count (rank=128): 89,267,465\n",
      "Shared attention parameter count: 63,264,521\n",
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Desktop/NLP/hw3/src/sharing_attention.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8780' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8780/8780 04:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.161913</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.039381</td>\n",
       "      <td>0.006334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.167100</td>\n",
       "      <td>0.183987</td>\n",
       "      <td>0.697733</td>\n",
       "      <td>0.792494</td>\n",
       "      <td>0.742101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.128112</td>\n",
       "      <td>0.835739</td>\n",
       "      <td>0.867385</td>\n",
       "      <td>0.851268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.135716</td>\n",
       "      <td>0.847582</td>\n",
       "      <td>0.890946</td>\n",
       "      <td>0.868723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.127297</td>\n",
       "      <td>0.862203</td>\n",
       "      <td>0.892965</td>\n",
       "      <td>0.877315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.128568</td>\n",
       "      <td>0.868472</td>\n",
       "      <td>0.901212</td>\n",
       "      <td>0.884539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Finished ---\n",
      "\n",
      "========================================\n",
      "           EXPERIMENT RESULTS\n",
      "========================================\n",
      "\n",
      "--- Final Evaluation on Test Set ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on the unseen test set:\n",
      "  - Test loss: 0.2731\n",
      "  - Test precision: 0.8119\n",
      "  - Test recall: 0.8596\n",
      "  - Test f1: 0.8351\n",
      "  - Test runtime: 3.5745\n",
      "  - Test samples_per_second: 965.9960\n",
      "  - Test steps_per_second: 120.8540\n"
     ]
    }
   ],
   "source": [
    "import sharing_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72418f0e-c123-4418-9e55-b613f99230d1",
   "metadata": {},
   "source": [
    "### Sharing attention + FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959d6008-6059-4837-b258-044730548573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Dataset ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Model and Applying Factorization ---\n",
      "Original parameter count: 107,726,601\n",
      "Factorized parameter count (rank=128): 89,267,465\n",
      "Shared attention+FFN parameter count: 11,300,873\n",
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Desktop/venv/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8780' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8780/8780 04:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.226384</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.064625</td>\n",
       "      <td>0.010450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.174065</td>\n",
       "      <td>0.727983</td>\n",
       "      <td>0.799899</td>\n",
       "      <td>0.762248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.143106</td>\n",
       "      <td>0.835001</td>\n",
       "      <td>0.856782</td>\n",
       "      <td>0.845751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.125932</td>\n",
       "      <td>0.837997</td>\n",
       "      <td>0.887075</td>\n",
       "      <td>0.861838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.127369</td>\n",
       "      <td>0.854990</td>\n",
       "      <td>0.888085</td>\n",
       "      <td>0.871223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.138545</td>\n",
       "      <td>0.858991</td>\n",
       "      <td>0.893975</td>\n",
       "      <td>0.876134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1026 22:31:10.252000 64701 torch/fx/experimental/symbolic_shapes.py:6833] [0/1] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "W1026 22:31:37.171000 64701 torch/fx/experimental/symbolic_shapes.py:6833] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "W1026 22:32:19.762000 64701 torch/fx/experimental/symbolic_shapes.py:6833] [0/3] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "W1026 22:34:10.103000 64701 torch/fx/experimental/symbolic_shapes.py:6833] [0/4] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Finished ---\n",
      "\n",
      "========================================\n",
      "           EXPERIMENT RESULTS\n",
      "========================================\n",
      "\n",
      "--- Final Evaluation on Test Set ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on the unseen test set:\n",
      "  - Test loss: 0.2848\n",
      "  - Test precision: 0.8006\n",
      "  - Test recall: 0.8481\n",
      "  - Test f1: 0.8237\n",
      "  - Test runtime: 3.5312\n",
      "  - Test samples_per_second: 977.8590\n",
      "  - Test steps_per_second: 122.3390\n"
     ]
    }
   ],
   "source": [
    "import sharing_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0291c2a-f9bc-489d-ad94-ba25c298458f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-7.2949e-02,  6.6831e-02, -1.1975e-02,  ..., -8.1796e-05,\n",
       "          1.0096e-02, -4.1468e-03],\n",
       "        [-7.3800e-02,  6.4152e-02, -1.1101e-02,  ...,  7.1646e-03,\n",
       "          6.3050e-03,  4.8796e-04],\n",
       "        [-7.3510e-02,  6.4133e-02, -1.0057e-02,  ...,  7.8719e-03,\n",
       "          5.8717e-03, -3.4092e-03],\n",
       "        ...,\n",
       "        [-6.2735e-02,  6.3804e-02, -4.6157e-03,  ...,  5.6066e-03,\n",
       "          1.4495e-02, -2.2848e-02],\n",
       "        [-6.2270e-02,  6.0139e-02, -4.8962e-03,  ..., -4.3955e-03,\n",
       "          1.4510e-02, -7.3002e-03],\n",
       "        [-6.4505e-02,  6.2615e-02, -5.9869e-03,  ...,  9.2165e-04,\n",
       "          1.2618e-02, -1.0796e-02]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharing_all.model.bert.embeddings.word_embeddings.embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c4012-90b3-44a9-9a81-cae65fa91f50",
   "metadata": {},
   "source": [
    "## Layer factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "242e9821-1bea-4a10-9226-dde8e9f070c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Dataset ---\n",
      "--- Loading Model and Applying Factorization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original parameter count: 107,726,601\n",
      "Factorized parameter count (rank=64): 87,362,569\n",
      "Factorized attention parameter count: 78,567,433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Desktop/NLP/hw3/src/layer_factorization.py:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8780' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8780/8780 05:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.217063</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.031976</td>\n",
       "      <td>0.010741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.251841</td>\n",
       "      <td>0.525147</td>\n",
       "      <td>0.676540</td>\n",
       "      <td>0.591307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>0.175056</td>\n",
       "      <td>0.661788</td>\n",
       "      <td>0.776170</td>\n",
       "      <td>0.714430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.162740</td>\n",
       "      <td>0.684747</td>\n",
       "      <td>0.813699</td>\n",
       "      <td>0.743675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.168767</td>\n",
       "      <td>0.743921</td>\n",
       "      <td>0.834063</td>\n",
       "      <td>0.786417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.170867</td>\n",
       "      <td>0.751158</td>\n",
       "      <td>0.846348</td>\n",
       "      <td>0.795917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Finished ---\n",
      "\n",
      "========================================\n",
      "           EXPERIMENT RESULTS\n",
      "========================================\n",
      "\n",
      "--- Final Evaluation on Test Set ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on the unseen test set:\n",
      "  - Test loss: 0.2991\n",
      "  - Test precision: 0.6992\n",
      "  - Test recall: 0.8052\n",
      "  - Test f1: 0.7485\n",
      "  - Test runtime: 4.3314\n",
      "  - Test samples_per_second: 797.2030\n",
      "  - Test steps_per_second: 99.7370\n"
     ]
    }
   ],
   "source": [
    "import layer_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4d600-af1f-45ac-91c7-357ca47d2687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93839c45-6007-4acb-9241-37c00631d72b",
   "metadata": {},
   "source": [
    "На практике качетсво получилось сильно хуже, чем я ожидал. Особенно учитывая, что модель потеряла не так много параметров по сравнению с другими методами.\n",
    "\n",
    "У меня почему-то не получилось факторизовать dense слой, так как модель начинала ооочень медленно учиться. Обучение можно дополнительно ускорить, если сфьюзить перемножение матриц внутри в одну операцию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e94376-c64c-4b84-a500-b85e53c3a1e7",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104f92f-f5a3-44c8-8799-29def2de941f",
   "metadata": {},
   "source": [
    "## Base distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6a56d8-67e4-41ea-975c-d808f620b62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Dataset ---\n",
      "--- Loading Model and Applying Factorization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/user/Desktop/venv/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher parameter count: 107,726,601\n",
      "\n",
      "--- Starting Teacher Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4390' max='4390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4390/4390 04:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.319321</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>0.063783</td>\n",
       "      <td>0.012702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.074012</td>\n",
       "      <td>0.907499</td>\n",
       "      <td>0.924605</td>\n",
       "      <td>0.915972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.062878</td>\n",
       "      <td>0.927498</td>\n",
       "      <td>0.945136</td>\n",
       "      <td>0.936234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.058581</td>\n",
       "      <td>0.922862</td>\n",
       "      <td>0.946314</td>\n",
       "      <td>0.934441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.070320</td>\n",
       "      <td>0.929960</td>\n",
       "      <td>0.949680</td>\n",
       "      <td>0.939717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.069582</td>\n",
       "      <td>0.932399</td>\n",
       "      <td>0.951700</td>\n",
       "      <td>0.941951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1026 23:19:18.552000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/1] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "W1026 23:19:41.789000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/2] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "W1026 23:20:05.748000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/3] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n",
      "W1026 23:20:50.589000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/4] _maybe_guard_rel() was called on non-relation expression Eq(s33, s92) | Eq(s92, 1)\n",
      "W1026 23:20:50.591000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/4] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Finished ---\n",
      "Student model created with 16,277,641 parameters.\n",
      "Student model configuration: BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "\n",
      "--- Starting Student Training ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 02:32, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>245.535706</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.065971</td>\n",
       "      <td>0.014957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>39.116600</td>\n",
       "      <td>49.013470</td>\n",
       "      <td>0.269460</td>\n",
       "      <td>0.326826</td>\n",
       "      <td>0.295384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>24.607400</td>\n",
       "      <td>31.562082</td>\n",
       "      <td>0.501724</td>\n",
       "      <td>0.612083</td>\n",
       "      <td>0.551437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16.546600</td>\n",
       "      <td>28.075096</td>\n",
       "      <td>0.550265</td>\n",
       "      <td>0.645742</td>\n",
       "      <td>0.594193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.341500</td>\n",
       "      <td>24.386417</td>\n",
       "      <td>0.594698</td>\n",
       "      <td>0.705991</td>\n",
       "      <td>0.645583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>13.766100</td>\n",
       "      <td>21.508753</td>\n",
       "      <td>0.677946</td>\n",
       "      <td>0.755301</td>\n",
       "      <td>0.714536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>12.104300</td>\n",
       "      <td>20.632029</td>\n",
       "      <td>0.695559</td>\n",
       "      <td>0.764389</td>\n",
       "      <td>0.728352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.030800</td>\n",
       "      <td>20.543615</td>\n",
       "      <td>0.708501</td>\n",
       "      <td>0.768596</td>\n",
       "      <td>0.737326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>6.652500</td>\n",
       "      <td>20.200182</td>\n",
       "      <td>0.718765</td>\n",
       "      <td>0.779367</td>\n",
       "      <td>0.747840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.285000</td>\n",
       "      <td>19.985102</td>\n",
       "      <td>0.717853</td>\n",
       "      <td>0.778862</td>\n",
       "      <td>0.747114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.829000</td>\n",
       "      <td>19.930532</td>\n",
       "      <td>0.729646</td>\n",
       "      <td>0.794850</td>\n",
       "      <td>0.760854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>5.702300</td>\n",
       "      <td>19.740454</td>\n",
       "      <td>0.720884</td>\n",
       "      <td>0.790643</td>\n",
       "      <td>0.754154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.922900</td>\n",
       "      <td>19.883739</td>\n",
       "      <td>0.729490</td>\n",
       "      <td>0.790138</td>\n",
       "      <td>0.758604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1026 23:23:26.360000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/5] _maybe_guard_rel() was called on non-relation expression Eq(s33, s92) | Eq(s92, 1)\n",
      "W1026 23:23:26.363000 92806 torch/fx/experimental/symbolic_shapes.py:6833] [0/5] _maybe_guard_rel() was called on non-relation expression Eq(s16, 1) | Eq(s27, s16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Finished ---\n",
      "\n",
      "========================================\n",
      "           EXPERIMENT RESULTS\n",
      "========================================\n",
      "\n",
      "--- Final Evaluation on Test Set ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics on the unseen test set:\n",
      "  - Test loss: 25.8285\n",
      "  - Test precision: 0.6627\n",
      "  - Test recall: 0.7130\n",
      "  - Test f1: 0.6869\n",
      "  - Test runtime: 2.0627\n",
      "  - Test samples_per_second: 1674.0250\n",
      "  - Test steps_per_second: 26.1790\n"
     ]
    }
   ],
   "source": [
    "import distillation_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a54eda-d44c-4c0d-a018-6a1f538a6e03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attention distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8257b-f360-47f2-adbf-2c6917d02072",
   "metadata": {},
   "source": [
    "#### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd906cc-c979-4405-a425-ea81932030d9",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59814f2-7498-4645-b3c2-a1cb191e100e",
   "metadata": {},
   "source": [
    "### Результаты оптимизации моделей:\n",
    "\n",
    "| Метод                 | Параметры (млн) | F1 (Test) | Runtime (Test) |\n",
    "|-----------------------|-----------------|-----------|----------------|\n",
    "| Embedding Factorization| 87.36           | 0.8713    | 3.90           |\n",
    "| Attention Sharing     | 63.26           | 0.8351    | 3.57           |\n",
    "| Attention + FFN Sharing| 11.30           | 0.8237    | 3.53           |\n",
    "| Layer Factorization   | 78.57           | 0.7485    | 4.33           |\n",
    "| Teacher (Distillation)| 107.72          | 0.9420    | -              |\n",
    "| Student (Distillation)| 16.27           | 0.7586    | 2.06           |\n",
    "\n",
    "*   **Embedding Factorization:** Лучший баланс между размером и качеством (F1 0.8713 при 87.36M параметров).\n",
    "*   **Attention + FFN Sharing:** Максимальное сокращение параметров (11.30M), но с F1 0.8237.\n",
    "*   **Layer Factorization:** Наихудшие показатели F1 среди факторизаций (0.7485). Фокус с низкоранговым приближением всех линейных слоев не прошел.\n",
    "*   **Distillation:** Student модель очень компактная и быстрая (16.27M, 2.06с), но с F1 0.7586 - требуется доработка."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02be8e4f604f4c98a728f298c4713f7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "08e5d6ea6e7e4e5c9c82be7209b10eef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_31909be8e29143d6922b75c1b49df7f6",
       "style": "IPY_MODEL_4d4fea80c6e14a8f8c59f6707e256e2b",
       "value": "Saving the dataset (1/1 shards): 100%"
      }
     },
     "0dba417763834d3e95f8f60f6c2c32f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "13b3dea34cd24b85be40546124ab695f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_48a69164cffb4dffba469d0ac0ac654d",
       "style": "IPY_MODEL_2c1fda273772494f8127c2abac3d67fe",
       "value": " 14041/14041 [00:00&lt;00:00, 376613.75 examples/s]"
      }
     },
     "1b40fc368407417fa07f447d71a403a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a6651d609fc04c4eb2724b9ce95b8977",
        "IPY_MODEL_a154197b370b473ea08ced663ac2641e",
        "IPY_MODEL_13b3dea34cd24b85be40546124ab695f"
       ],
       "layout": "IPY_MODEL_8579a7844bd0462e9155ca3a84eeed93"
      }
     },
     "21d6f52831ae4553bfc3a55823a56a8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2c1fda273772494f8127c2abac3d67fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "31909be8e29143d6922b75c1b49df7f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "429e125735714e26b9afa2e29c7e289c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_da265713851d40e6868ba6a6a4a9c10a",
       "style": "IPY_MODEL_fee293e8cfbd4e51b892314b707bb87d",
       "value": "Saving the dataset (1/1 shards): 100%"
      }
     },
     "482807f2c1414ba7a59fa73a279ce4a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0dba417763834d3e95f8f60f6c2c32f6",
       "style": "IPY_MODEL_a7ab73b00bda4d068fe0df9312096327",
       "value": " 3250/3250 [00:00&lt;00:00, 171635.81 examples/s]"
      }
     },
     "48a69164cffb4dffba469d0ac0ac654d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "49533a2145814cfcbe253724e13fa77c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d4fea80c6e14a8f8c59f6707e256e2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4f77c27de93e44d6b285bc926f9b47e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_08e5d6ea6e7e4e5c9c82be7209b10eef",
        "IPY_MODEL_d2085b20a2ad4156a57a07667c1c5490",
        "IPY_MODEL_560b7c2b2d624b649d827bae7fceedfe"
       ],
       "layout": "IPY_MODEL_e1b4df45c38c4b5db3c06f4a9b21d384"
      }
     },
     "560b7c2b2d624b649d827bae7fceedfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_73b1be1cd80849d280ef6c5a96c952ad",
       "style": "IPY_MODEL_b26360a7be1144198ca0faa19def0ca7",
       "value": " 3453/3453 [00:00&lt;00:00, 186946.49 examples/s]"
      }
     },
     "565d876158a24c0b8ccb47ad2058977d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "73b1be1cd80849d280ef6c5a96c952ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8579a7844bd0462e9155ca3a84eeed93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a154197b370b473ea08ced663ac2641e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a8fe3b67cf974e21baeb39b6c2ecc147",
       "max": 14041,
       "style": "IPY_MODEL_d9140e98a0384402bd900edd72048e30",
       "value": 14041
      }
     },
     "a22297e4c6494a129419897024be661d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a6651d609fc04c4eb2724b9ce95b8977": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b9ca6c8b82df426e9d9dd5268247b898",
       "style": "IPY_MODEL_a22297e4c6494a129419897024be661d",
       "value": "Saving the dataset (1/1 shards): 100%"
      }
     },
     "a7ab73b00bda4d068fe0df9312096327": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a8fe3b67cf974e21baeb39b6c2ecc147": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b1e8ae952f85462b9e2deb2f141b715e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b26360a7be1144198ca0faa19def0ca7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b9ca6c8b82df426e9d9dd5268247b898": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d2085b20a2ad4156a57a07667c1c5490": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_02be8e4f604f4c98a728f298c4713f7e",
       "max": 3453,
       "style": "IPY_MODEL_b1e8ae952f85462b9e2deb2f141b715e",
       "value": 3453
      }
     },
     "d9140e98a0384402bd900edd72048e30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "da265713851d40e6868ba6a6a4a9c10a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "db55adb7820e47dc84a61fcedbd8dfe9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_429e125735714e26b9afa2e29c7e289c",
        "IPY_MODEL_f9a02b0a0120428b8a72a5f3250643d8",
        "IPY_MODEL_482807f2c1414ba7a59fa73a279ce4a6"
       ],
       "layout": "IPY_MODEL_49533a2145814cfcbe253724e13fa77c"
      }
     },
     "e1b4df45c38c4b5db3c06f4a9b21d384": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f9a02b0a0120428b8a72a5f3250643d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_565d876158a24c0b8ccb47ad2058977d",
       "max": 3250,
       "style": "IPY_MODEL_21d6f52831ae4553bfc3a55823a56a8e",
       "value": 3250
      }
     },
     "fee293e8cfbd4e51b892314b707bb87d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
